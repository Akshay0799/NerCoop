{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P0lmrRrqik5"
      },
      "outputs": [],
      "source": [
        "!pip install skweak "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eefBIF7s2Wv",
        "outputId": "b66bf87d-5c4a-4964-a844-b2ad418a6bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNLbUwVgK-IS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NorskRegnesentral/skweak.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Er4ResLXtl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # data = pd.read_csv(\"wnut17.txt\", sep=\"\\t\").fillna(method=\"ffill\")\n",
        "# data = pd.read_csv(\"wikigold.conll.txt\", sep=\"\\t\").fillna(method=\"ffill\")\n",
        "# # data = pd.read_csv(\"btca.conll\", sep=\"\\t\").fillna(method=\"ffill\")\n",
        "# # data['tag'][0] = 'B-Chemical'\n",
        "# data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading and Tokenizing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Kq1DBTKuQP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import itertools \n",
        "\n",
        "def get_tokens_and_ner_tags(filename):\n",
        "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
        "        tokens = [[x.split('\\t')[0] for x in y] for y in split_list]\n",
        "        entities = [[x.split('\\t')[1][:-1] for x in y] for y in split_list] \n",
        "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
        "\n",
        "train_doc = get_tokens_and_ner_tags('wnut17.txt')\n",
        "# train_doc = get_tokens_and_ner_tags('btca.conll')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfX6YaqANByc"
      },
      "outputs": [],
      "source": [
        "docs = train_doc.tokens\n",
        "lines = []\n",
        "for d in docs:\n",
        "  line = \"\"\n",
        "  for i in d:\n",
        "    line = line+i+\" \"\n",
        "    # print(i)\n",
        "  # print(line)\n",
        "  lines.append(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7MtvceLKwtc"
      },
      "outputs": [],
      "source": [
        "lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIuw_gjCOL2h"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable, Tuple\n",
        "import re, json, os\n",
        "# import snips_nlu_parsers\n",
        "from skweak.base import CombinedAnnotator, SpanAnnotator\n",
        "from skweak.spacy import ModelAnnotator, TruecaseAnnotator\n",
        "from skweak.heuristics import FunctionAnnotator, TokenConstraintAnnotator, SpanConstraintAnnotator, SpanEditorAnnotator\n",
        "from skweak.gazetteers import GazetteerAnnotator, extract_json_data\n",
        "from skweak.doclevel import DocumentHistoryAnnotator, DocumentMajorityAnnotator\n",
        "from skweak.aggregation import MajorityVoter\n",
        "from skweak import utils\n",
        "from spacy.tokens import Doc, Span\n",
        "import spacy\n",
        "import skweak\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmz9qhUScx47"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU5fxZnFaXbw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        " \n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])\n",
        "docs = list(nlp.pipe(lines))\n",
        "# docs = skweak.utils.docbin_reader(lines)\n",
        "# docs = nlp(lines[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofOQqzHVFmzD"
      },
      "source": [
        "Detect Company Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyusTHd3aBrV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def company_detector_fun(doc):\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if chunk[-1].lower_.rstrip(\".\") in {'corp', 'inc', 'ltd', 'llc', 'sa', 'ag'}:\n",
        "            yield chunk.start, chunk.end, \"ORG\"\n",
        "\n",
        "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fun)\n",
        "docs = list(company_detector.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"company_detector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8NW6W6RhoNp",
        "outputId": "d5ee2624-bcfb-4f32-db72-30a978ef47de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Plan to spend a fabulous day in Discover Downtown La Grange for their 5th Annual Chocolate Crawl on a walkable ... http://t.co/lbhSVeriLZ "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss1uUqNWFpN3"
      },
      "source": [
        "Other Organisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o69up9IjaB2U"
      },
      "outputs": [],
      "source": [
        "OTHER_ORG_CUE_WORDS = {\"University\", \"Institute\", \"College\", \"Committee\", \"Party\", \"Agency\",\n",
        "                       \"Union\", \"Association\", \"Organization\", \"Court\", \"Office\", \"National\"}\n",
        "def other_org_detector_fun(doc):\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if any([tok.text in OTHER_ORG_CUE_WORDS for tok in chunk]):\n",
        "            yield chunk.start, chunk.end, \"ORG\"\n",
        "\n",
        "\n",
        "other_org_detector = skweak.heuristics.FunctionAnnotator(\"other_org_detector\", other_org_detector_fun)\n",
        "docs = list(other_org_detector.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"other_org_detector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wj3HIv9cqL2"
      },
      "source": [
        "Detection of Proper nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TpuUG_7b1d_"
      },
      "outputs": [],
      "source": [
        "proper_detector = skweak.heuristics.TokenConstraintAnnotator(\"proper_detector\", skweak.utils.is_likely_proper, \"ENT\")\n",
        "        \n",
        "\n",
        "nnp_detector = skweak.heuristics.TokenConstraintAnnotator(\"nnp_detector\", lambda tok: tok.tag_==\"NNP\", \"ENT\")\n",
        "\n",
        "compound = lambda tok: skweak.utils.is_likely_proper(tok) and skweak.utils.in_compound(tok)\n",
        "compound_detector = skweak.heuristics.TokenConstraintAnnotator(\"compound_detector\", compound, \"ENT\")\n",
        " \n",
        "combined = skweak.base.CombinedAnnotator()\n",
        "\n",
        "for annotator in [proper_detector, nnp_detector, compound_detector]:\n",
        "    annotator.add_gap_tokens([\"'s\", \"-\"])\n",
        "    combined.add_annotator(annotator)\n",
        "\n",
        "   \n",
        "    infrequent_name = \"infrequent_%s\"%annotator.name\n",
        "    combined.add_annotator(skweak.heuristics.SpanConstraintAnnotator(infrequent_name, annotator.name, skweak.utils.is_infrequent))\n",
        "\n",
        "docs = list(combined.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"proper_detector\")\n",
        "skweak.utils.display_entities(docs[0], \"nnp_detector\")\n",
        "skweak.utils.display_entities(docs[0], \"compound_detector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud-lip4Ci5gC"
      },
      "source": [
        "Detecting Misc Entities - Doesn't work here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90aVIrVKi4M-"
      },
      "outputs": [],
      "source": [
        "def misc_generator(doc):\n",
        "    \"\"\"Detects occurrences of countries and various less-common entities (NORP, FAC, EVENT, LANG)\"\"\"\n",
        "    \n",
        "    # spans = set(doc.spans[\"proper2_detector\"])\n",
        "    spans |= {doc[i:i+1] for i in range(len(doc))}\n",
        "    \n",
        "    for span in sorted(spans):\n",
        "\n",
        "        span_text = span.text\n",
        "        if span_text.isupper():\n",
        "            span_text = span_text.title()\n",
        "        last_token = doc[span.end-1].text\n",
        "\n",
        "        if span_text in data_utils.COUNTRIES:\n",
        "            yield span.start, span.end, \"GPE\"\n",
        "\n",
        "        if len(span) <= 3 and (span in data_utils.NORPS or last_token in data_utils.NORPS \n",
        "                               or last_token.rstrip(\"s\") in data_utils.NORPS):\n",
        "            yield span.start, span.end, \"NORP\"\n",
        "    \n",
        "        if span in data_utils.LANGUAGES and doc[span.start].tag_==\"NNP\":\n",
        "            yield span.start, span.end, \"LANGUAGE\"\n",
        "            \n",
        "        if last_token in data_utils.FACILITIES and len(span) > 1:\n",
        "            yield span.start, span.end, \"FAC\"     \n",
        "\n",
        "        if last_token in data_utils.EVENTS  and len(span) > 1:\n",
        "            yield span.start, span.end, \"EVENT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evo9DA4bhP33"
      },
      "source": [
        "Detect name using Regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch8_b_wihSKQ"
      },
      "outputs": [],
      "source": [
        "def name_detector(doc):\n",
        "    \"\"\"Searches for occurrences of time patterns in text\"\"\"\n",
        "\n",
        "    i = 0\n",
        "    while i < len(doc):\n",
        "        tok = doc[i]\n",
        "\n",
        "        if re.match(\"r'^[a-z ,.\\'-]+$'\", tok.text):\n",
        "            yield i, i + 1, \"NAME\"\n",
        "            i += 1\n",
        "        i += 1\n",
        "name_annotator = skweak.heuristics.FunctionAnnotator(\"name_annotator\", name_detector)\n",
        "docs = list(name_annotator.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"name_annotator\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzLSLVPJ82hD"
      },
      "source": [
        "## Gazeteers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwyalZh886oV"
      },
      "source": [
        "General Wiki Gazeteer for all entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag8XplOs1cjM"
      },
      "outputs": [],
      "source": [
        "tries = skweak.gazetteers.extract_json_data(\"data/wikidata_small_tokenised.json\")\n",
        "annotator = skweak.gazetteers.GazetteerAnnotator(\"wiki\", tries)\n",
        "docs = list(annotator.pipe(docs))\n",
        "# annotator(docs[0])\n",
        "skweak.utils.display_entities(docs[0], \"wiki\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyohFJpN8z9z"
      },
      "source": [
        "Location Entity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmDogRV41_Wd"
      },
      "outputs": [],
      "source": [
        "tries = skweak.gazetteers.extract_json_data(\"data/geonames.json\",  spacy_model=\"en_core_web_sm\")\n",
        "annotator = skweak.gazetteers.GazetteerAnnotator(\"geo_cased\", tries)\n",
        "annotator2 = skweak.gazetteers.GazetteerAnnotator(\"geo_uncased\", tries, case_sensitive=False)\n",
        "docs = list(annotator2.pipe(list(annotator.pipe(docs))))\n",
        "\n",
        "skweak.utils.display_entities(docs[0], [\"geo_cased\", \"geo_uncased\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8AB1v8Fr6o"
      },
      "source": [
        "Detect Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4u-UoqfuGav"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "FIRST_NAMES = \"data/first_names.json\"\n",
        "class FullNameDetector():\n",
        "    \"\"\"Search for occurrences of full person names (first name followed by at least one title token)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        fd = open(FIRST_NAMES)\n",
        "        self.first_names = set(json.load(fd))\n",
        "        fd.close()\n",
        "\n",
        "    def __call__(self, span: Span) -> bool:\n",
        "        # We assume full names are between 2 and 5 tokens\n",
        "        if len(span) < 2 or len(span) > 5:\n",
        "            return False\n",
        "\n",
        "        return (span[0].text in self.first_names and\n",
        "                span[-1].is_alpha and span[-1].is_title)\n",
        "\n",
        "proper_detector = skweak.heuristics.TokenConstraintAnnotator(\"proper_detector\", skweak.utils.is_likely_proper, \"ENT\")\n",
        "\n",
        "full_name_detector = skweak.heuristics.SpanConstraintAnnotator(\"full_name_detector\", \"proper_detector\", FullNameDetector(), \"PERSON\")\n",
        "docs = list(full_name_detector.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"full_name_detector\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8sI55tjF1Hd"
      },
      "source": [
        "Using spacy model to detect entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBcUQzo_bzY3"
      },
      "outputs": [],
      "source": [
        "ner = skweak.spacy.ModelAnnotator(\"spacy\", \"en_core_web_sm\")\n",
        "docs = list(ner.pipe(docs))\n",
        "skweak.utils.display_entities(docs[0], \"spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPI5jfhHMbE3"
      },
      "outputs": [],
      "source": [
        "# hmm = skweak.generative.HMM(\"hmm\", [\"COMPANY\", \"PERSON\", \"DATE\", \"MONEY\", \"ORG\", \"LOCATION\", \"GPE\"])\n",
        "# hmm.fit([docs[0]]*5)\n",
        "# c = []\n",
        "\n",
        "# for doc in docs:\n",
        "#   doc = hmm(doc)\n",
        "#   c.append(doc)\n",
        "# docs = list(hmm.pipe(docs))\n",
        "# utils.display_entities(docs[0], \"hmm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axEwxbvX80cn"
      },
      "source": [
        "Aggregating the results and training the HMM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "9VhS36EWb_ZF",
        "outputId": "d07d5f18-3416-41be-cc97-d5a849c8db63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting iteration 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-923982efe556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# And run the estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/NER/skweak/skweak/aggregation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mobs_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_observation_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/NER/skweak/skweak/generative.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, all_obs, cutoff, n_iter, tol)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# Update the sufficient statistics based on the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mcurr_logprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulate_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mobs_i\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobs_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/NER/skweak/skweak/generative.py\u001b[0m in \u001b[0;36m_accumulate_statistics\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# Compute its current log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mframelogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# We run a forward and backward pass to compute the posteriors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/NER/skweak/skweak/generative.py\u001b[0m in \u001b[0;36m_get_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 logsum.max(axis=1).min() < -100000): # type: ignore  \n\u001b[1;32m    165\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid state found at position %i\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogsum\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No valid state found at position 30"
          ]
        }
      ],
      "source": [
        "model = skweak.aggregation.HMM(\"hmm\", [\"COMPANY\", \"PERSON\", \"GPE\", \"LOCATION\"])\n",
        "\n",
        "model.add_underspecified_label(\"ENT\", [\"LOC\", \"COMPANY\", \"ORG\", \"PER\"])\n",
        "\n",
        "# And run the estimation\n",
        "model.fit_and_aggregate(docs)\n",
        "skweak.utils.display_entities(docs[0], \"hmm\", add_tooltip=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p68Qq3RYMF66"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_annotator = examples.ner.conll2003_ner.NERAnnotator().add_all()\n",
        "docs = list(full_annotator.pipe(docs))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SkWeak.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
