
until 31st August: WIESP Shared task, report writing etc.  

Sep: Wrapping up adversarial testing, may be do adversarial fine-tuning experiments, writing up and submitting the paper to ACL Rolling Review.  

End of Sep, 1st week of Oct: Reading on cross-lingual transfer.  

Oct-Nov: explore cross-lingual transfer approaches for NER.  
(few shot or zero shot. Goal: how can we reuse NER model trained on one language on another language). 

***********

Notes on 17 Oct 2022:  
- Following approaches in [Sources of Transfer in Multilingual Named Entity Recognition](https://aclanthology.org/2020.acl-main.720) (Mueller et al., ACL 2020)
 and [Unsupervised cross-lingual model transfer for named entity recognition with contextualized word representations](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257230) (Yan et.al, Plos One, 2021) 
 with the [CONLL-2002 (Spanish, Dutch)](https://huggingface.co/datasets/conll2002) and [CONLL-2003 (English, German)](https://huggingface.co/datasets/conll2003) NER datasets
 
 (comparing monolingual models with multilingual models and multilingual+fine-tuned to a language models, zero-shot transfer with models trained on 1-n
  source languages, exploring adapter approaches (e.g., one large transformer model +adapters for each language NER) etc.  

- Extend the experiments to multiconer dataset.    

- Explore other architectures for NER (using transformer model embeddings as just initial step).   

- Check if using pre-trained modular transformers ([Pfeiffer et.al., NAACL 2022](https://aclanthology.org/2022.naacl-main.255.pdf)) + fine-tuning on NER task is any better than using regular pre-trained models+fine-tuning or pre-trained models+adapters  

  
  
  
***********
Dec: Wrapping up, writing up a report.  

Topics:
- Understanding transfer in NER
     - Cross-lingual transfer (zero-shot, few shot, how much training data do we need in cross-lingual transfer etc). 

- Improving robustness of NER models in cross-lingual setups.    
     - Approaches for making NER more generalizable.    
     - Fine-tuning methods for cross-lingual setups.   
     - Cross-lingual modular models etc https://aclanthology.org/2022.naacl-main.255/. 
